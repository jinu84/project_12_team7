# streamlit_rag_manual_chatbot.py

import streamlit as st
from dotenv import load_dotenv
import os
import tempfile
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatAnthropic
from langchain.docstore.document import Document
from langchain.chains.qa_with_sources import load_qa_with_sources_chain

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
claude_api_key = os.getenv("ANTHROPIC_API_KEY")

# Tesseract ì„¤ì¹˜ ê²½ë¡œ ì„¤ì • (ìœˆë„ìš°)
pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"

st.set_page_config(page_title="ğŸ“± ì „ìì œí’ˆ ì„¤ëª…ì„œ Claude ì±—ë´‡")
st.title("ğŸ“± ì „ìì œí’ˆ ì„¤ëª…ì„œ ê¸°ë°˜ Claude RAG ì±—ë´‡")
st.markdown("ì „ìì œí’ˆ PDF ì„¤ëª…ì„œë¥¼ ì—…ë¡œë“œí•˜ê³ , ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”. ì˜ˆ: *ë°°í„°ë¦¬ êµì²´ ë°©ë²•ì€?*")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "last_uploaded_file_name" not in st.session_state:
    st.session_state.last_uploaded_file_name = None

# PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ì „ìì œí’ˆ ì„¤ëª…ì„œ PDF ì—…ë¡œë“œ", type="pdf")

# OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_with_ocr(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        pix = page.get_pixmap()
        img = Image.open(io.BytesIO(pix.tobytes("png")))
        text += pytesseract.image_to_string(img, lang="kor+eng") + "\n"
    return text

# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• í•¨ìˆ˜
def build_vectorstore(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_text(text)
    documents = [Document(page_content=t) for t in texts]
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectorstore = FAISS.from_documents(documents, embeddings)
    return vectorstore

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì—…ë¡œë“œëœ íŒŒì¼ì´ ìƒˆ íŒŒì¼ì¼ ë•Œë§Œ ì²˜ë¦¬)
if uploaded_file and uploaded_file.name != st.session_state.last_uploaded_file_name:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name
    with st.spinner("OCR ë° ì„ë² ë”© ì²˜ë¦¬ ì¤‘..."):
        extracted_text = extract_text_with_ocr(tmp_path)
        vs = build_vectorstore(extracted_text)
        st.session_state.vectorstore = vs
        st.session_state.last_uploaded_file_name = uploaded_file.name
        st.success("PDF ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

# ì§ˆë¬¸ ì²˜ë¦¬
query = st.chat_input("ì „ìì œí’ˆ ì„¤ëª…ì„œì—ì„œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”")

if query and st.session_state.vectorstore:
    with st.spinner("Claudeê°€ ì„¤ëª…ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì¤‘..."):
        retriever = st.session_state.vectorstore.as_retriever()
        llm = ChatAnthropic(model_name="claude-3-haiku-20240307", temperature=0, anthropic_api_key=claude_api_key)
        chain = load_qa_with_sources_chain(llm, chain_type="stuff")
        docs = retriever.get_relevant_documents(query)
        result = chain({"input_documents": docs, "question": "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜: " + query}, return_only_outputs=True)
        st.chat_message("user").markdown(query)
        st.chat_message("assistant").markdown(result["output_text"])
elif query:
    st.warning("ë¨¼ì € ì„¤ëª…ì„œ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ê°¤ëŸ­ì‹œ ìš¸íŠ¸ë¼24 ìš¸íŠ¸ë¼.pdf')")
