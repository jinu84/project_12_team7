import os
import fitz  # PyMuPDF
import tempfile
import streamlit as st
from dotenv import load_dotenv
import re
import unicodedata

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_anthropic import ChatAnthropic

# ğŸ”§ ì¤„ë°”ê¿ˆ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_newlines(text: str) -> str:
    lines = text.split('\n')
    cleaned = []
    for i in range(len(lines)):
        line = lines[i].strip()
        if i == len(lines) - 1:
            cleaned.append(line)
            break
        if not re.search(r"[.!?â€¦]$|ìŠµë‹ˆë‹¤$|í•©ë‹ˆë‹¤$|ë˜ë©°$|ë©ë‹ˆë‹¤$", line):
            cleaned.append(line + ' ')
        else:
            cleaned.append(line + '\n')
    return ''.join(cleaned)

# âœ… ë³´ê¸° ë“œë¬¸ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì œê±° í•„í„°
def remove_unusual_unicode(text: str) -> str:
    return re.sub(r"[^\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F"  # í•œê¸€
                  r"\u0041-\u005A\u0061-\u007A"                 # ì˜ì–´
                  r"\u0030-\u0039"                              # ìˆ«ì
                  r"\u0020-\u007E"                              # ì¼ë°˜ íŠ¹ìˆ˜ê¸°í˜¸
                  r"\u3000-\u303F\uFF00-\uFFEF"                 # ì „ê° ê¸°í˜¸
                  r"\u2010-\u205E"                              # dash, quote ë“±
                  r"\n\r\t .,;:?!\"'()\-â€“â€”_=+/\\[\]{}<>%@&#~"    # ì¼ë°˜ ê¸°í˜¸
                  r"]+", "", text)

# ğŸ“¤ ì´ë¯¸ì§€ ì œê±°: í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì¶”ì¶œ
def extract_text_without_images(doc):
    cleaned_texts = []
    for page in doc:
        blocks = page.get_text("blocks")  # (x0, y0, x1, y1, text, block_no, type)
        text_blocks = [block[4] for block in blocks if block[6] == 0]  # block type 0 = text only
        page_text = "\n".join(text_blocks).strip()
        page_text = remove_unusual_unicode(page_text)  # ğŸ”¹ ì´ìƒí•œ ë¬¸ì ì œê±° ì ìš©
        cleaned_texts.append(page_text)
    return cleaned_texts

# ğŸŒ± í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY")

# ğŸ›ï¸ Streamlit UI
st.set_page_config(page_title="ğŸ“š PDF RAG vs LLM ë¹„êµ", page_icon="ğŸ“„")
st.title("ğŸ“˜ PDF ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ (RAG vs LLM ë¹„êµ)")

# ğŸ“‚ PDF ì—…ë¡œë“œ
uploaded_pdf = st.file_uploader("ğŸ“ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if uploaded_pdf:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_pdf.read())
        tmp_path = tmp_file.name

    doc = fitz.open(tmp_path)
    page_texts = extract_text_without_images(doc)
    doc.close()

    # ğŸ“ í˜ì´ì§€ë³„ ì›ë¬¸ í™•ì¸
    with st.expander("ğŸ“ ì „ì²´ íŒŒì‹±ëœ PDF í…ìŠ¤íŠ¸ (í˜ì´ì§€ë³„)"):
        for idx, text in enumerate(page_texts):
            with st.expander(f"ğŸ“„ Page {idx + 1}"):
                st.text(text)

    # ğŸ“Œ ì „ì²˜ë¦¬ + ì²­í¬ ìƒì„±
    with st.spinner("ğŸ” PDF ì²­í¬ ìƒì„± ì¤‘..."):
        merged_text = "\n".join(page_texts)
        cleaned_text = clean_newlines(merged_text)

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=400,
            separators=["\n\n", ".", " ", "\n"]
        )
        docs = text_splitter.create_documents([cleaned_text])

        with st.expander("ğŸ§© ìƒì„±ëœ ì²­í¬ í™•ì¸"):
            for idx, doc in enumerate(docs):
                with st.expander(f"ğŸ”¹ ì²­í¬ {idx + 1} (ê¸¸ì´: {len(doc.page_content)}ì)"):
                    st.text(doc.page_content)

        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
        vectorstore = FAISS.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10})

        prompt = PromptTemplate.from_template("""
        ë„ˆëŠ” ì—…ë¡œë“œëœ PDF ë¬¸ì„œì— ê¸°ë°˜í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
        ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´.

        [ë¬¸ë§¥]
        {context}

        [ì§ˆë¬¸]
        {question}

        [ë‹µë³€]
        """)

        llm = ChatAnthropic(model="claude-3-haiku-20240307", api_key=CLAUDE_API_KEY, temperature=0)

        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

    if question:
        with st.spinner("ğŸ¤– RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘..."):
            rag_response = rag_chain.invoke(question)

        with st.spinner("ğŸ§  LLM ë‹¨ë… ì‘ë‹µ ìƒì„± ì¤‘..."):
            direct_prompt = f"""
            ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë¬¸ì„œ ì—†ì´ë„ ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
            ì§ˆë¬¸ì— ëŒ€í•´ ì•Œê³  ìˆëŠ” ì§€ì‹ìœ¼ë¡œ ìµœëŒ€í•œ ì„±ì‹¤í•˜ê²Œ ë‹µí•´ì¤˜.

            [ì§ˆë¬¸]
            {question}

            [ë‹µë³€]
            """
            llm_response = llm.invoke(direct_prompt)

        st.markdown("### ğŸ” RAG ê¸°ë°˜ ì‘ë‹µ")
        st.success(f"**Claude + PDF Context:**\n\n{rag_response}")

        st.markdown("### ğŸ§  LLM ë‹¨ë… ì‘ë‹µ")
        st.info(f"**Claude ë‹¨ë… ì‘ë‹µ:**\n\n{llm_response}")
