import streamlit as st  # âœ… ë°˜ë“œì‹œ í•„ìš”

# ğŸ“‚ ì‚¬ì´ë“œë°” ë©”ë‰´ êµ¬ì„±
menu = st.sidebar.radio("ğŸ“‚ ë©”ë‰´ ì„ íƒ", ["ğŸ’¬ Chatbot", "ğŸ“Š ìœ ì‚¬ë„ ë¹„êµ"])

if menu == "ğŸ’¬ Chatbot":
    # ì—¬ê¸° ì•ˆì— Chatbot ì½”ë“œ ì „ì²´ ë³µì‚¬í•´ì„œ ë„£ê¸°
    # ì§€ê¸ˆê¹Œì§€ ì‘ì„±í•˜ì‹  ëª¨ë“  ì½”ë“œ ê·¸ëŒ€ë¡œ!
    
    import os
    import fitz  # PyMuPDF
    import tempfile

    from dotenv import load_dotenv
    import re
    import hashlib

    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_core.prompts import PromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
    from langchain_community.chat_models import ChatOpenAI
    from langchain.schema import Document


    # ğŸ”§ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def clean_newlines(text: str) -> str:
        lines = text.split('\n')
        cleaned = []
        for i in range(len(lines)):
            line = lines[i].strip()
            if i == len(lines) - 1:
                cleaned.append(line)
                break
            if not re.search(r"[.!?â€¦]$|ìŠµë‹ˆë‹¤$|í•©ë‹ˆë‹¤$|ë˜ë©°$|ë©ë‹ˆë‹¤$", line):
                cleaned.append(line + ' ')
            else:
                cleaned.append(line + '\n')
        return ''.join(cleaned)

    def remove_unusual_unicode(text: str) -> str:
        return re.sub(r"[^\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F"
                    r"\u0041-\u005A\u0061-\u007A"
                    r"\u0030-\u0039"
                    r"\u0020-\u007E"
                    r"\u3000-\u303F\uFF00-\uFFEF"
                    r"\u2010-\u205E"
                    r"\n\r\t .,;:?!\"'()\-\u2013\u2014_=+/\\[\]{}<>%@&#~"
                    r"]+", "", text)

    def extract_text_without_images(doc):
        cleaned_texts = []
        for page in doc:
            blocks = page.get_text("blocks")
            text_blocks = [block[4] for block in blocks if block[6] == 0]
            page_text = "\n".join(text_blocks).strip()
            page_text = remove_unusual_unicode(page_text)
            cleaned_texts.append(page_text)
        return cleaned_texts

    def calculate_file_hash(file_path):
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    # ğŸŒ± í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # ğŸŒ Streamlit í˜ì´ì§€ ì„¤ì •
    st.set_page_config(page_title="ğŸ“š PDF ê¸°ë°˜ ì±—ë´‡ (Claude + RAG)", page_icon="ğŸ“„")
    st.title("ğŸ“˜ PDF ê¸°ë°˜ ì±—ë´‡ (Claude + RAG)")

    # âœ… ì œí’ˆ ë¶„ë¥˜ ì •ì˜
    product_categories = {
        "ëª¨ë°”ì¼": ["ìŠ¤ë§ˆíŠ¸í°", "íœ´ëŒ€í°"],
        "TV/ì˜ìƒ.ìŒí–¥": ["í…”ë ˆë¹„ì „", "TV", "ì‚¬ìš´ë“œë°”"],
        "ê°€ì „": ["ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°", "ì—ì–´ë“œë ˆì„œ", "ì²­ì†Œê¸°", "ê±´ì¡°ê¸°"],
        "PC / í”„ë¦°í„°": ["ì»´í“¨í„°", "ë…¸íŠ¸ë¶", "í”„ë¦°í„°", "ë³µí•©ê¸°"],
        "ë©”ëª¨ë¦¬ & ìŠ¤í† ë¦¬ì§€": ["ë©”ëª¨ë¦¬", "SSD", "microSD", "ìŠ¤í† ë¦¬ì§€"],
        "ë””ìŠ¤í”Œë ˆì´": ["ëª¨ë‹ˆí„°", "ë””ìŠ¤í”Œë ˆì´"],
        "ì¹´ë©”ë¼ & ìº ì½”ë”": ["ì¹´ë©”ë¼", "ìº ì½”ë”"]
    }

    # ğŸ“ ì—¬ëŸ¬ PDF ì—…ë¡œë“œ
    uploaded_pdfs = st.file_uploader("ğŸ“ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)

    highlighted = set()
    category_to_products = {}
    product_to_text = {}
    all_page_texts = []

    # ìƒíƒœ ì´ˆê¸°í™”
    if "selected_category" not in st.session_state:
        st.session_state.selected_category = None
    if "selected_product" not in st.session_state:
        st.session_state.selected_product = None

    # ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ë° ì¹´í…Œê³ ë¦¬/ì œí’ˆëª… ì¶”ì¶œ
    if uploaded_pdfs:
        for uploaded_pdf in uploaded_pdfs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(uploaded_pdf.read())
                tmp_path = tmp_file.name

            doc = fitz.open(tmp_path)
            page_texts = extract_text_without_images(doc)
            doc.close()

            merged_text = "\n".join(page_texts)
            cleaned_text = clean_newlines(merged_text)
            all_page_texts.extend(page_texts)

            llm = ChatOpenAI(
                model="gpt-4o",
                temperature=0,
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
            # ì œí’ˆ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
            category_prompt = PromptTemplate.from_template("""
            ì•„ë˜ëŠ” ì œí’ˆ ë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤:
            {categories}

            ìœ„ ë¶„ë¥˜ ì¤‘ ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš©ì— ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì œí’ˆ ë¶„ë¥˜ í•˜ë‚˜ë§Œ ì •í™•íˆ ê³¨ë¼ì£¼ì„¸ìš”.

            [ë¬¸ì„œ ë‚´ìš© ìš”ì•½]
            {text}

            [ê´€ë ¨ ì œí’ˆ ë¶„ë¥˜]""")
            category_chain = (
                {"categories": lambda x: ", ".join(product_categories.keys()), "text": lambda x: cleaned_text[:1500]}
                | category_prompt
                | llm
                | StrOutputParser()
            )
            category_response = category_chain.invoke("dummy")

            matched_category = None
            for label in product_categories:
                if label in category_response:
                    highlighted.add(label)
                    matched_category = label
                    break

            # ì œí’ˆëª… ì¶”ë¡ 
            name_prompt = PromptTemplate.from_template("""
            ì•„ë˜ëŠ” ì „ìì œí’ˆ ì„¤ëª…ì„œì˜ ë³¸ë¬¸ ë‚´ìš© ì¼ë¶€ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ì½ê³  ì œí’ˆëª…ì„ ì •í™•íˆ í•œ ì¤„ë¡œ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: ê°¤ëŸ­ì‹œ S24 ìš¸íŠ¸ë¼, ì‚¼ì„± ë¹„ìŠ¤í¬í¬ ì„¸íƒê¸° ë“±

            [ì„¤ëª…ì„œ ë‚´ìš©]
            {text}

            [ì œí’ˆëª…]""")
            name_chain = (
                {"text": lambda x: cleaned_text[:2000]}
                | name_prompt
                | llm
                | StrOutputParser()
            )
            name_response = name_chain.invoke("dummy").strip()

            if matched_category:
                if name_response not in category_to_products.get(matched_category, []):
                    category_to_products.setdefault(matched_category, []).append(name_response)
                    product_to_text[name_response] = cleaned_text

    # ğŸ“¦ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼
    st.subheader("ğŸ“¦ ì œí’ˆ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”")
    items = [
        ("ğŸ“±", "ëª¨ë°”ì¼"), ("ğŸ“º", "TV/ì˜ìƒ.ìŒí–¥"),
        ("ğŸ§º", "ê°€ì „"), ("ğŸ’»", "PC / í”„ë¦°í„°"),
        ("ğŸ’¾", "ë©”ëª¨ë¦¬ & ìŠ¤í† ë¦¬ì§€"), ("ğŸ–¥ï¸", "ë””ìŠ¤í”Œë ˆì´"),
        ("ğŸ“·", "ì¹´ë©”ë¼ & ìº ì½”ë”")
    ]
    rows = [items[i:i+4] for i in range(0, len(items), 4)]
    for row in rows:
        cols = st.columns(len(row), gap="small")
        for col, (icon, label) in zip(cols, row):
            with col:
                if label in highlighted:
                    if st.button(f"{icon}\n{label}", key=label, use_container_width=True):
                        st.session_state.selected_category = label
                else:
                    st.button(f"{icon}\n{label}", key=label, use_container_width=True, disabled=True)

    # ğŸ·ï¸ ì œí’ˆëª… ì„ íƒ
    selected_category = st.session_state.selected_category
    if selected_category and selected_category in category_to_products:
        st.subheader("ğŸ·ï¸ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì œí’ˆëª…")
        selected_product = st.selectbox(
            "ì œí’ˆëª…ì„ ì„ íƒí•˜ì„¸ìš”",
            category_to_products[selected_category],
            index=category_to_products[selected_category].index(st.session_state.selected_product)
            if st.session_state.selected_product in category_to_products[selected_category]
            else 0,
            key="selected_product"  # ì£¼ì˜: key ì„¤ì • í›„ ì™¸ë¶€ì—ì„œ ì´ ê°’ì„ setí•˜ì§€ ë§ ê²ƒ
        )


    # ğŸ’¬ ì±—ë´‡ ì˜ì—­
    if st.session_state.get("selected_product") not in [None, ""]:
        selected_product = st.session_state.selected_product
        product_text = product_to_text[selected_product]

        # ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.create_documents([product_text])
        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
        vectorstore = FAISS.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§· í•¨ìˆ˜
        def format_chat_history(chat_pairs):
            return "\n".join([f"{'ì‚¬ìš©ì' if role == 'user' else 'ì±—ë´‡'}: {msg}" for role, msg in chat_pairs[:-1]])

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt = PromptTemplate.from_template("""
        ë„ˆëŠ” ì—…ë¡œë“œëœ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
        ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ê°€ì¥ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•´ì¤˜.
        ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì¤˜.

        [ëŒ€í™” íˆìŠ¤í† ë¦¬]
        {chat_history}

        [ì§ˆë¬¸]
        {question}

        [ë¬¸ë§¥]
        {context}

        [ë‹µë³€]
        """)

        # LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # RAG ì²´ì¸
        rag_chain = (
            {
                "question": lambda h: h[-1][1],
                "chat_history": lambda h: format_chat_history(h),
                "context": lambda h: retriever.invoke(h[-1][1])
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        if question := st.chat_input("ğŸ’¬ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”!"):
            st.session_state.chat_history.append(("user", question))
            with st.spinner("ğŸ¤– RAG ì‘ë‹µ ìƒì„± ì¤‘..."):
                answer = rag_chain.invoke(st.session_state.chat_history)
            st.session_state.chat_history.append(("ai", answer))

        # ì±„íŒ… í‘œì‹œ
        for role, msg in st.session_state.chat_history:
            with st.chat_message(role):
                st.markdown(
                    f"<div style='background-color: {'#DCF8C6' if role == 'user' else '#E6ECF0'}; "
                    f"padding: 10px; border-radius: 8px'>{msg}</div>", unsafe_allow_html=True
                )

            # âœ… ê¸°ë³¸ GPT-4o ì‘ë‹µ ë³´ê¸°
        if "chat_history" in st.session_state and len(st.session_state.chat_history) >= 2:
            last_user_msg = st.session_state.chat_history[-2][1]  # ì§ì „ ì‚¬ìš©ì ì§ˆë¬¸
            with st.expander("ğŸ¤– ê¸°ë³¸ GPT-4o ì‘ë‹µ ë³´ê¸°"):
                base_llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    openai_api_key=os.getenv("OPENAI_API_KEY")
                )

                base_prompt = PromptTemplate.from_template("""
                ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¼ë°˜ì ì¸ GPT-4o ëª¨ë¸ë¡œ ë‹µë³€í•˜ì„¸ìš”.
                ë¬¸ì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ì¼ë°˜ ì§€ì‹ì— ê¸°ë°˜í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

                [ì§ˆë¬¸]
                {question}

                [ë‹µë³€]
                """)

                base_chain = (
                    {"question": lambda _: last_user_msg}
                    | base_prompt
                    | base_llm
                    | StrOutputParser()
                )

                base_answer = base_chain.invoke("dummy")  # ì…ë ¥ê°’ì€ ë¬´ì‹œë¨
                st.markdown(base_answer)

                # âœ… ê¸°ë³¸ GPT-3.5 ì‘ë‹µ ë³´ê¸°
        if "chat_history" in st.session_state and len(st.session_state.chat_history) >= 2:
            last_user_msg = st.session_state.chat_history[-2][1]  # ì§ì „ ì‚¬ìš©ì ì§ˆë¬¸
            with st.expander("ğŸ¤– ê¸°ë³¸ GPT-3.5 ì‘ë‹µ ë³´ê¸°"):
                gpt35_llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    openai_api_key=os.getenv("OPENAI_API_KEY")
                )

                gpt35_prompt = PromptTemplate.from_template("""
                ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¼ë°˜ì ì¸ GPT-3.5 ëª¨ë¸ë¡œ ë‹µë³€í•˜ì„¸ìš”.
                ë¬¸ì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ì¼ë°˜ ì§€ì‹ì— ê¸°ë°˜í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

                [ì§ˆë¬¸]
                {question}

                [ë‹µë³€]
                """)

                gpt35_chain = (
                    {"question": lambda _: last_user_msg}
                    | gpt35_prompt
                    | gpt35_llm
                    | StrOutputParser()
                )

                gpt35_answer = gpt35_chain.invoke("dummy")  # ì…ë ¥ê°’ì€ ë¬´ì‹œë¨
                st.markdown(gpt35_answer)


        # âœ… ì²­í¬ ë¯¸ë¦¬ë³´ê¸°ëŠ” ì—¬ê¸°! ë£¨í”„ ë°–ì— ìˆì–´ì•¼ í•­ìƒ ë³´ì…ë‹ˆë‹¤
        with st.expander("ğŸ“„ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°"):
            preview_chunks = text_splitter.split_text(product_text)
            for i, chunk in enumerate(preview_chunks):
                st.markdown(f"**Chunk {i+1}**")
                st.code(chunk)

                from sentence_transformers import SentenceTransformer, util

if menu == "ğŸ“Š ìœ ì‚¬ë„ ë¹„êµ":
    st.subheader("ğŸ“Š ì‘ë‹µ ìœ ì‚¬ë„ ë¹„êµ (BERT ê¸°ë°˜)")

    # ì‘ë‹µ ê°€ì ¸ì˜¤ê¸° (ì´ì „ ì½”ë“œì—ì„œ ì €ì¥ëœ ë‚´ìš©)
    rag_answer = st.session_state.chat_history[-1][1] if st.session_state.chat_history else ""
    gpt4o_answer = st.session_state.get("gpt4o_base_answer", "")
    gpt35_answer = st.session_state.get("gpt35_base_answer", "")

    if rag_answer and gpt4o_answer and gpt35_answer:
        model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

        rag_emb = model.encode(rag_answer, convert_to_tensor=True)
        gpt4o_emb = model.encode(gpt4o_answer, convert_to_tensor=True)
        gpt35_emb = model.encode(gpt35_answer, convert_to_tensor=True)

        sim_rag_gpt4o = util.cos_sim(rag_emb, gpt4o_emb).item()
        sim_rag_gpt35 = util.cos_sim(rag_emb, gpt35_emb).item()
        sim_gpt4o_gpt35 = util.cos_sim(gpt4o_emb, gpt35_emb).item()

        st.markdown("#### âœ… ì‘ë‹µ ê°„ ìœ ì‚¬ë„ (Cosine Similarity)")
        st.write(f"ğŸ“˜ **RAG vs GPT-4o**: {sim_rag_gpt4o:.4f}")
        st.write(f"ğŸ“˜ **RAG vs GPT-3.5**: {sim_rag_gpt35:.4f}")
        st.write(f"ğŸ“˜ **GPT-4o vs GPT-3.5**: {sim_gpt4o_gpt35:.4f}")
    else:
        st.warning("RAG ë˜ëŠ” ê¸°ë³¸ GPT ì‘ë‹µì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ì±—ë´‡ì—ì„œ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")


