import os
import fitz  # PyMuPDF
import tempfile
import streamlit as st
import hashlib
import re
from dotenv import load_dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_anthropic import ChatAnthropic
from langchain.schema import Document

# ======================= ì „ì²˜ë¦¬ í•¨ìˆ˜ =======================

def remove_unusual_unicode(text: str) -> str:
    return re.sub(r"[^\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F"
                  r"\u0041-\u005A\u0061-\u007A"
                  r"\u0030-\u0039"
                  r"\u0020-\u007E"
                  r"\u3000-\u303F\uFF00-\uFFEF"
                  r"\u2010-\u205E"
                  r"\n\r\t .,;:?!\"'()\-\u2013\u2014_=+/\\[\]{}<>%@&#~"
                  r"]+", "", text)

def clean_newlines(text: str) -> str:
    lines = text.split('\n')
    cleaned = []
    for i in range(len(lines)):
        line = lines[i].strip()
        if i == len(lines) - 1:
            cleaned.append(line)
            break
        if not re.search(r"[.!?â€¦]$|ìŠµë‹ˆë‹¤$|í•©ë‹ˆë‹¤$|ë˜ë©°$|ë©ë‹ˆë‹¤$", line):
            cleaned.append(line + ' ')
        else:
            cleaned.append(line + '\n')
    return ''.join(cleaned)

def extract_text_without_images(doc):
    cleaned_texts = []
    for page in doc:
        blocks = page.get_text("blocks")
        text_blocks = [block[4] for block in blocks if block[6] == 0]
        page_text = "\n".join(text_blocks).strip()
        page_text = remove_unusual_unicode(page_text)
        cleaned_texts.append(page_text)
    return cleaned_texts

def calculate_file_hash(file_path):
    sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256.update(chunk)
    return sha256.hexdigest()

# ======================= í™˜ê²½ ë³€ìˆ˜ ë° ì´ˆê¸° ì„¤ì • =======================

load_dotenv()
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY")

st.set_page_config(page_title="ğŸ“š PDF ì„ íƒ ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ", page_icon="ğŸ“„")
st.title("ğŸ“˜ ì„ íƒí•œ PDF ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ (RAG vs LLM)")

uploaded_pdfs = st.file_uploader("ğŸ“ ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)

# ======================= ì²˜ë¦¬ ë¡œì§ =======================

if uploaded_pdfs:
    pdf_name_to_vectorstore = {}
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")

    os.makedirs("faiss_index", exist_ok=True)

    for uploaded_pdf in uploaded_pdfs:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_pdf.read())
            tmp_path = tmp_file.name

        file_hash = calculate_file_hash(tmp_path)
        index_path = os.path.join("faiss_index", file_hash)

        if os.path.exists(f"{index_path}/index.faiss") and os.path.exists(f"{index_path}/index.pkl"):
            vectorstore = FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True)
        else:
            doc = fitz.open(tmp_path)
            page_texts = extract_text_without_images(doc)
            doc.close()

            cleaned_text = clean_newlines("\n".join(page_texts))
            docs = text_splitter.create_documents([cleaned_text])

            # âœ… ID ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìƒˆ Documentë¡œ ìƒì„±
            docs = [Document(page_content=d.page_content, metadata=d.metadata) for d in docs]

            vectorstore = FAISS.from_documents(docs, embeddings)
            vectorstore.save_local(index_path)

        pdf_name_to_vectorstore[uploaded_pdf.name] = vectorstore

    # ======================= ì§ˆë¬¸ ì„¹ì…˜ =======================

    selected_pdf_name = st.selectbox("ğŸ“„ ì§ˆë¬¸í•  PDFë¥¼ ì„ íƒí•˜ì„¸ìš”", list(pdf_name_to_vectorstore.keys()))
    question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if question and selected_pdf_name:
        selected_vectorstore = pdf_name_to_vectorstore[selected_pdf_name]
        retriever = selected_vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10})

        # RAG í”„ë¡¬í”„íŠ¸
        prompt = PromptTemplate.from_template("""
        ë„ˆëŠ” ì—…ë¡œë“œëœ PDF ë¬¸ì„œì— ê¸°ë°˜í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
        ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´.

        [ë¬¸ë§¥]
        {context}

        [ì§ˆë¬¸]
        {question}

        [ë‹µë³€]
        """)

        llm = ChatAnthropic(model="claude-3-haiku-20240307", api_key=CLAUDE_API_KEY, temperature=0)

        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        with st.spinner("ğŸ¤– RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘..."):
            rag_response = rag_chain.invoke(question)

        with st.spinner("ğŸ§  LLM ë‹¨ë… ì‘ë‹µ ìƒì„± ì¤‘..."):
            direct_prompt = f"""
            ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë¬¸ì„œ ì—†ì´ë„ ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
            ì§ˆë¬¸ì— ëŒ€í•´ ì•Œê³  ìˆëŠ” ì§€ì‹ìœ¼ë¡œ ìµœëŒ€í•œ ì„±ì‹¤í•˜ê²Œ ë‹µí•´ì¤˜.

            [ì§ˆë¬¸]
            {question}

            [ë‹µë³€]
            """
            llm_response = llm.invoke(direct_prompt)

        st.markdown(f"### ğŸ” RAG ì‘ë‹µ (ğŸ“„ {selected_pdf_name})")
        st.success(rag_response)

        st.markdown("### ğŸ§  LLM ë‹¨ë… ì‘ë‹µ")
        st.info(llm_response)
