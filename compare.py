import os
import fitz  # PyMuPDF
import tempfile
import streamlit as st
from dotenv import load_dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain.chat_models import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CLAUDE_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# ğŸ›ï¸ Streamlit UI ì„¤ì •
st.set_page_config(page_title="ğŸ“š PDF ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ ë¹„êµ", page_icon="ğŸ“„")
st.title("ğŸ“˜ PDF ê¸°ë°˜ RAG vs OpenAI vs Claude ë¹„êµ ì‘ë‹µ")

uploaded_pdf = st.file_uploader("ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if uploaded_pdf:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_pdf.read())
        tmp_path = tmp_file.name

    doc = fitz.open(tmp_path)
    page_texts = [page.get_text() for page in doc]
    doc.close()

    # ğŸ” í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=400,
        separators=["\n\n", "\n", ".", " "]
    )
    all_texts = "\n".join(page_texts)
    docs = text_splitter.create_documents([all_texts])

    # ğŸ’¡ ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10})

    # ğŸ§  ê³µí†µ í”„ë¡¬í”„íŠ¸
    prompt = PromptTemplate.from_template("""
    ë„ˆëŠ” ì—…ë¡œë“œëœ PDF ë¬¸ì„œì— ê¸°ë°˜í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
    ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´.

    [ë¬¸ë§¥]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """)

    # LLM ì„¸íŒ…
    llm_claude = ChatAnthropic(model="claude-3-haiku-20240307", api_key=CLAUDE_API_KEY, temperature=0)
    llm_openai = ChatOpenAI(model="gpt-3.5-turbo", api_key=OPENAI_API_KEY, temperature=0)

    rag_claude_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_claude
        | StrOutputParser()
    )

    rag_openai_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_openai
        | StrOutputParser()
    )

    # âœ¨ ì‘ë‹µ ìƒì„±
    if question:
        with st.spinner("ğŸ¤– Claude + RAG ì‘ë‹µ ìƒì„± ì¤‘..."):
            response_rag_claude = rag_claude_chain.invoke(question)

        with st.spinner("ğŸ¤– OpenAI + RAG ì‘ë‹µ ìƒì„± ì¤‘..."):
            response_rag_openai = rag_openai_chain.invoke(question)

        with st.spinner("ğŸ§  Claude ë‹¨ë… ì‘ë‹µ ìƒì„± ì¤‘..."):
            direct_prompt = f"""
            ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë¬¸ì„œ ì—†ì´ë„ ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ì•¼.
            ì§ˆë¬¸ì— ëŒ€í•´ ì•Œê³  ìˆëŠ” ì§€ì‹ìœ¼ë¡œ ìµœëŒ€í•œ ì„±ì‹¤í•˜ê²Œ ë‹µí•´ì¤˜.

            [ì§ˆë¬¸]
            {question}

            [ë‹µë³€]
            """
            direct_response = llm_claude.invoke(direct_prompt)

        # ğŸ“ ì¶œë ¥
        st.subheader("ğŸ” ì‘ë‹µ ë¹„êµ ê²°ê³¼")
        st.markdown("#### ğŸ“˜ Claude + PDF RAG")
        st.success(response_rag_claude)

        st.markdown("#### ğŸ¤– OpenAI + PDF RAG")
        st.info(response_rag_openai)

        st.markdown("#### ğŸ’¬ Claude (ë¬¸ì„œ ë¯¸í¬í•¨)")
        st.warning(direct_response)